from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio import SeqIO, bgzf
import gzip
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import re
from tqdm import tqdm
tqdm.pandas(desc="my bar!")


genomes = pd.read_csv("genome_list.tsv", header=None).values.ravel()
print(genomes)

features = ["CDS", "five_prime_UTR", "three_prime_UTR"]
splits = ["train", "test"]
split_chromosomes = {
    "train": ["Chr1", "Chr2", "Chr3", "Chr4"],
    "test": ["Chr5",],
}

MIN_CONTIG_LEN = 512


rule all:
    input:
        "genomes/all.contigs.parquet",
        #"windows/val/1000/100/seqs.txt",
        expand("windows/{feature}.test/512/128/seqs.txt", feature=features),
        expand("dataset/{feature}.parquet", feature=features),


rule download_reference:
    output:
        "tair10.raw.fa",
    shell:
        "wget https://www.arabidopsis.org/download_files/Genes/TAIR10_genome_release/TAIR10_chromosome_files/TAIR10_chr_all.fas -O {output}"


rule clean_reference:
    input:
        "tair10.raw.fa",
    output:
        "tair10.fa",
    run:
        records = []
        with open(input[0]) as input_handle:
            for record in SeqIO.parse(input_handle, "fasta"):
                print(record.id)
                if record.id in ["M", "chloroplast"]: continue
                record.id = "Chr" + record.id
                print(record.id)
                records.append(record)

        with open(output[0], "w") as output_handle:
            SeqIO.write(records, output_handle, "fasta")


rule make_windows_lm_raw:
    input:
       "intervals/{split}.bed"
    output:
        "windows/{split}/{window_size}/{step_size}/raw.bed"
    shell:
        "bedtools makewindows -b {input} -w {wildcards.window_size} -s {wildcards.step_size} | awk '$3-$2=={wildcards.window_size}' > {output}"


rule make_windows_lm_pos:
    input:
        "{anything}/raw.bed"
    output:
        "{anything}/pos.bed"
    shell:
        """awk '{{print $0 "\t.\t.\t+"}}' {input} > {output}"""


rule make_windows_lm_neg:
    input:
        "{anything}/raw.bed"
    output:
        "{anything}/neg.bed"
    shell:
        """awk '{{print $0 "\t.\t.\t-"}}' {input} > {output}"""


rule make_windows_lm:
    input:
        "{anything}/pos.bed",
        "{anything}/neg.bed",
    output:
        "{anything}/all.bed",
    shell:
        "cat {input} > {output}"


rule extract_seq_lm:
    input:
        "{anything}/all.bed",
        "tair10.fa",
    output:
        "{anything}/seqs.txt",
    shell:
        """bedtools getfasta -fi {input[1]} -bed {input[0]} -s | grep -v "[^ACTG]" > {output}"""


rule filter_fasta_train:
    input:
        "tair10.fa",
        "intervals/train.bed",
    output:
        "tair10.train.fa",
    run:
        intervals = pd.read_csv(input[1], sep="\t", header=None, names=["chromosome", "start", "end"]).set_index("chromosome")

        records = []
        with open(input[0]) as input_handle:
            for record in SeqIO.parse(input_handle, "fasta"):
                if record.id in intervals.index.values:
                    records.append(record[intervals.loc[record.id, "start"]:intervals.loc[record.id, "end"]])

        with open(output[0], "w") as output_handle:
            SeqIO.write(records, output_handle, "fasta")


rule split_into_contigs:
    input:
        "{anything}.fa.gz",
    output:
        "{anything}.contigs.parquet",
    run:
        with gzip.open(input[0], "rt") as handle:
            sequence = '\n'.join([str(record.seq).strip() for record in SeqIO.parse(handle, "fasta")])
        m = re.sub('[^ACGTacgt]+', '\n', sequence).split('\n')
        m = [x for x in m if len(x) >= MIN_CONTIG_LEN]
        df = pd.DataFrame(dict(
            contig_id=[f"contig_{i}" for i in range(len(m))],
            seq=m,
        ))
        df["species"] = wildcards["anything"]
        df["contig_len"] = df.seq.str.len()
        print(df)
        df.to_parquet(output[0], index=False)


rule concat_genomes:
    input:
        expand("genomes/{genome}.contigs.parquet", genome=genomes),
    output:
        "genomes/all.contigs.parquet",
    run:
        df = pd.concat([pd.read_parquet(input_path) for input_path in input], ignore_index=True)
        print(df)
        df.species.replace({"genomes/Arabidopsis_thaliana_train": "genomes/AAA_Arabidopsis_thaliana_train"}, inplace=True)  # to ensure it's sorted first
        df["species_id"] = pd.Categorical(df.species).codes
        print(df)
        print(df[df.species_id==0])
        df.to_parquet(output[0], index=False)


rule find_specific_regions:
    input:
        "../vep/tair10.gff",
        #"../vep/tss.bed",
    output:
        expand("intervals/{feature}.{split}.bed", feature=features, split=splits)
        #"intervals/promoter.bed",
    run:
        gtf = pd.read_csv(
            input[0], sep="\t", header=None, comment="#",
            names=['chromosome', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute']
        )
        print(gtf)

        chromosomes = ["Chr1", "Chr2", "Chr3", "Chr4", "Chr5"]
        gtf = gtf[gtf.chromosome.isin(chromosomes)]

        gtf.start -= MIN_CONTIG_LEN // 2
        gtf.end += MIN_CONTIG_LEN // 2

        for split in splits:
            print(split)
            gtf_split = gtf[gtf.chromosome.isin(split_chromosomes[split])]
            for feature in features:
                print(feature)
                gtf_split[gtf_split.feature==feature][["chromosome", "start", "end"]].drop_duplicates().to_csv(f"intervals/{feature}.{split}.bed", sep="\t", index=False, header=False)
        # TODO remove duplicate positions in tss.bed


rule make_train_data_region:
    input:
        "intervals/{feature}.train.bed",
        "tair10.fa",
        "genomes/Arabidopsis_thaliana_train.contigs.parquet",
    output:
        "dataset/{feature}.parquet",
    run:
        df = pd.read_csv(input[0], sep="\t", header=None, names=["chromosome", "start", "end"])
        print(df)
        genome = SeqIO.to_dict(SeqIO.parse(input[1], "fasta"))
        df["seq"] = df.apply(lambda row: str(genome[row.chromosome][row.start:row.end].seq), axis=1)
        print(df)
        df["contig_id"] = df.chromosome.astype(str) + ":" + df.start.astype(str) + "-" + df.end.astype(str)
        df.drop(columns=["chromosome", "start", "end"], inplace=True)
        df["contig_len"] = df.seq.str.len()
        print(df)
        print(df.contig_len.describe())

        df["contig_type"] = "target"

        df2 = pd.read_parquet(input[2])
        df2["contig_type"] = "background"

        df = pd.concat([df, df2], ignore_index=True)
        print(df)
        print(df.contig_type.value_counts())

        df["contig_weight"] = (1 + df.contig_len - MIN_CONTIG_LEN).clip(lower=1)

        contig_weight_by_type = df.groupby("contig_type").contig_weight.sum()
        print(contig_weight_by_type)
        print(contig_weight_by_type / contig_weight_by_type.sum())

        target_ratio = 0.5
        weight_multiplier = (contig_weight_by_type["background"]*target_ratio) / (contig_weight_by_type["target"]*(1-target_ratio))
        print(weight_multiplier)

        df.loc[df.contig_type=="target", "contig_weight"] *= weight_multiplier
        contig_weight_by_type = df.groupby("contig_type").contig_weight.sum()
        print(contig_weight_by_type)
        print(contig_weight_by_type / contig_weight_by_type.sum())

        df.to_parquet(output[0], index=False)