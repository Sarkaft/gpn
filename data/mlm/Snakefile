from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio import SeqIO, bgzf
import gzip
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import re
from tqdm import tqdm
tqdm.pandas()


genomes = pd.read_csv("genome_list.tsv", header=None).values.ravel()
print(genomes)

features = ["CDS", "five_prime_UTR", "three_prime_UTR"]
splits = ["train", "test"]
split_chromosomes = {
    "train": ["Chr1", "Chr2", "Chr3", "Chr4"],
    "test": ["Chr5",],
}


REPEATS_CUT_AWAY_FROM_BORDER = 128
MIN_CONTIG_LEN = 512


rule all:
    input:
        #"genomes/all.spike_target_0.5.contigs.parquet",
        #"genomes/all.contigs.parquet",
        "windows/val/512/256/seqs.txt",
        #expand("windows/{feature}.test/512/128/seqs.txt", feature=features),
        #expand("dataset/{feature}.parquet", feature=features),


rule download_reference:
    output:
        "tair10.raw.fa",
    shell:
        "wget http://ftp.ensemblgenomes.org/pub/plants/release-53/fasta/arabidopsis_thaliana/dna/Arabidopsis_thaliana.TAIR10.dna_sm.toplevel.fa.gz -O - | gunzip -c > {output}"


rule clean_reference:
    input:
        "tair10.raw.fa",
    output:
        "tair10.fa",
    run:
        records = []
        with open(input[0]) as input_handle:
            for record in SeqIO.parse(input_handle, "fasta"):
                print(record.id)
                if record.id in ["M", "chloroplast", "Mt", "Pt"]: continue
                record.id = "Chr" + record.id
                print(record.id)
                records.append(record)

        with open(output[0], "w") as output_handle:
            SeqIO.write(records, output_handle, "fasta")


rule make_windows_lm_raw:
    input:
       "intervals/{split}.bed"
    output:
        "windows/{split}/{window_size}/{step_size}/raw.bed"
    shell:
        "bedtools makewindows -b {input} -w {wildcards.window_size} -s {wildcards.step_size} | awk '$3-$2=={wildcards.window_size}' > {output}"


rule make_windows_lm_pos:
    input:
        "{anything}/raw.bed"
    output:
        "{anything}/pos.bed"
    shell:
        """awk '{{print $0 "\t.\t.\t+"}}' {input} > {output}"""


rule make_windows_lm_neg:
    input:
        "{anything}/raw.bed"
    output:
        "{anything}/neg.bed"
    shell:
        """awk '{{print $0 "\t.\t.\t-"}}' {input} > {output}"""


rule make_windows_lm:
    input:
        "{anything}/pos.bed",
        "{anything}/neg.bed",
    output:
        "{anything}/all.bed",
    shell:
        "cat {input} > {output}"


rule extract_seq_lm:
    input:
        "{anything}/all.bed",
        "tair10.fa",
    output:
        "{anything}/seqs.txt",
    shell:
        """bedtools getfasta -fi {input[1]} -bed {input[0]} -s | grep -v "[^ACGTacgt]" > {output}"""


# non-repetitive and non-N
rule find_good_intervals:
    input:
        "tair10.fa",
    output:
        "intervals/Ath.bed",
    shell:
        "python find_good_intervals.py {input} {output} {REPEATS_CUT_AWAY_FROM_BORDER} {MIN_CONTIG_LEN}"


rule split_good_intervals:
    input:
        "intervals/Ath.bed",
    output:
        expand("intervals/Ath.{split}.bed", split=splits),
    run:
        intervals = pd.read_csv(input[0], sep="\t", header=None, names=["chrom", "start", "end"])
        for split, path in zip(splits, output):
            intervals[intervals.chrom.isin(split_chromosomes[split])].to_csv(path, sep="\t", index=False, header=False)


rule make_train_dataset:
    input:
        "tair10.fa",
        "intervals/Ath.train.bed",
    output:
        "dataset/Ath.train.parquet",
    run:
        genome = SeqIO.to_dict(SeqIO.parse(input[0], "fasta"))
        df = pd.read_csv(input[1], sep="\t", header=None, names=["chromosome", "start", "end"])
        print(df)
        df["contig_id"] = df.chromosome.astype(str) + ":" + df.start.astype(str) + "-" + df.end.astype(str)
        df["seq"] = df.apply(lambda row: str(genome[row.chromosome][row.start:row.end].seq), axis=1)
        print(df)
        df["contig_len"] = df.seq.str.len()
        print(df)
        df.to_parquet(output[0], index=False)


rule make_test_dataset:
    input:
        "tair10.fa",
        "intervals/Ath.test.bed",
    output:
        "dataset/Ath.test.{window_size}.{step_size}.parquet",
    run:
        genome = SeqIO.to_dict(SeqIO.parse(input[0], "fasta"))
        contigs = pd.read_csv(input[1], sep="\t", header=None, names=["chromosome", "start", "end"])

        window_size = int(wildcards["window_size"])
        step_size = int(wildcards["step_size"])

        def get_contig_windows(contig):
            windows = pd.DataFrame(dict(start=np.arange(contig.start, contig.end-window_size, step_size)))
            windows["end"] = windows.start + window_size
            windows["chromosome"] = contig.chromosome
            windows["strand"] = "+"
            windows_neg = windows.copy()
            windows_neg.strand = "-"
            windows = pd.concat([windows, windows_neg], ignore_index=True)
            return windows

        windows = pd.concat(contigs.apply(get_contig_windows, axis=1).values, ignore_index=True)
        print(windows)

        def get_window_seq(window):
            seq = genome[window.chromosome][window.start:window.end].seq
            if window.strand == "-":
                seq = seq.reverse_complement()
            return str(seq)

        windows["seq"] = windows.progress_apply(get_window_seq, axis=1)
        print(windows)
        windows.to_parquet(output[0])


"""rule concat_genomes:
    input:
        expand("genomes/{genome}.contigs.parquet", genome=genomes),
    output:
        "genomes/all.contigs.parquet",
    run:
        df = pd.concat([pd.read_parquet(input_path) for input_path in input], ignore_index=True)
        print(df)
        df.species.replace({"genomes/Arabidopsis_thaliana_train": "genomes/AAA_Arabidopsis_thaliana_train"}, inplace=True)  # to ensure it's sorted first
        df["species_id"] = pd.Categorical(df.species).codes
        print(df)
        print(df[df.species_id==0])
        df.to_parquet(output[0], index=False)"""


rule find_specific_regions:
    input:
        "../vep/tair10.gff",
        #"../vep/tss.bed",
    output:
        expand("intervals/{feature}.{split}.bed", feature=features, split=splits)
        #"intervals/promoter.bed",
    run:
        gtf = pd.read_csv(
            input[0], sep="\t", header=None, comment="#",
            names=['chromosome', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute']
        )
        print(gtf)

        chromosomes = ["Chr1", "Chr2", "Chr3", "Chr4", "Chr5"]
        gtf = gtf[gtf.chromosome.isin(chromosomes)]

        gtf.start -= MIN_CONTIG_LEN // 2
        gtf.end += MIN_CONTIG_LEN // 2

        for split in splits:
            print(split)
            gtf_split = gtf[gtf.chromosome.isin(split_chromosomes[split])]
            for feature in features:
                print(feature)
                gtf_split[gtf_split.feature==feature][["chromosome", "start", "end"]].drop_duplicates().to_csv(f"intervals/{feature}.{split}.bed", sep="\t", index=False, header=False)
        # TODO remove duplicate positions in tss.bed


rule make_train_data_region:
    input:
        "intervals/{feature}.train.bed",
        "tair10.fa",
        "genomes/Arabidopsis_thaliana_train.contigs.parquet",
    output:
        "dataset/{feature}.parquet",
    run:
        df = pd.read_csv(input[0], sep="\t", header=None, names=["chromosome", "start", "end"])
        print(df)
        genome = SeqIO.to_dict(SeqIO.parse(input[1], "fasta"))
        df["seq"] = df.apply(lambda row: str(genome[row.chromosome][row.start:row.end].seq), axis=1)
        print(df)
        df["contig_id"] = df.chromosome.astype(str) + ":" + df.start.astype(str) + "-" + df.end.astype(str)
        df.drop(columns=["chromosome", "start", "end"], inplace=True)
        df["contig_len"] = df.seq.str.len()
        print(df)
        print(df.contig_len.describe())

        df["contig_type"] = "target"

        df2 = pd.read_parquet(input[2])
        df2["contig_type"] = "background"

        df = pd.concat([df, df2], ignore_index=True)
        print(df)
        print(df.contig_type.value_counts())

        df["contig_weight"] = (1 + df.contig_len - MIN_CONTIG_LEN).clip(lower=1)

        contig_weight_by_type = df.groupby("contig_type").contig_weight.sum()
        print(contig_weight_by_type)
        print(contig_weight_by_type / contig_weight_by_type.sum())

        target_ratio = 0.5
        weight_multiplier = (contig_weight_by_type["background"]*target_ratio) / (contig_weight_by_type["target"]*(1-target_ratio))
        print(weight_multiplier)

        df.loc[df.contig_type=="target", "contig_weight"] *= weight_multiplier
        contig_weight_by_type = df.groupby("contig_type").contig_weight.sum()
        print(contig_weight_by_type)
        print(contig_weight_by_type / contig_weight_by_type.sum())

        df.to_parquet(output[0], index=False)


rule make_train_data_spike_arabidopsis:
    input:
        "genomes/all.contigs.parquet",
    output:
        "genomes/all.spike_target_{target_ratio}.contigs.parquet",
    run:
        df = pd.read_parquet(input[0])
        df["contig_type"] = "background"
        df.loc[df.species_id==0, "contig_type"] = "target"

        df["contig_weight"] = (1 + df.contig_len - MIN_CONTIG_LEN).clip(lower=1)

        contig_weight_by_type = df.groupby("contig_type").contig_weight.sum()
        print(contig_weight_by_type)
        print(contig_weight_by_type / contig_weight_by_type.sum())

        target_ratio = float(wildcards["target_ratio"])
        weight_multiplier = (contig_weight_by_type["background"]*target_ratio) / (contig_weight_by_type["target"]*(1-target_ratio))
        print(weight_multiplier)

        df.loc[df.contig_type=="target", "contig_weight"] *= weight_multiplier
        contig_weight_by_type = df.groupby("contig_type").contig_weight.sum()
        print(contig_weight_by_type)
        print(contig_weight_by_type / contig_weight_by_type.sum())

        df.to_parquet(output[0], index=False)


rule make_latex_table_train_sizes:
    input:
        "genomes/all.contigs.parquet",
    output:
        "table_train_sizes.txt",
    run:
        df = pd.read_parquet(input[0])
        df = df.groupby("species").contig_len.sum().to_frame().reset_index()
        print(df)
        df.species = df.species.str.replace("genomes/", "").str.replace("_", " ")
        df.species.replace({"AAA Arabidopsis thaliana train": "Arabidopsis thaliana"}, inplace=True)
        df.species = "\textit{" + df.species + "}"
        print(df)
        print("Total: ", df.contig_len.sum()) 
        df.contig_len = (df.contig_len / 1000000).round(decimals=0).astype(int).astype(str) + " Mb"
        print(df)
        df.to_latex(output[0], index=False, escape=False)
