import anndata as ad
from Bio import SeqIO
from Bio.Seq import Seq
from datasets import load_dataset
import h5py
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
import tempfile
import torch.nn as nn
from tqdm import tqdm
from transformers import AutoTokenizer, Trainer, TrainingArguments

from plantbert.mlm.convnet import ConvNetModel


CONTEXT = 1000  # total context twice this


metadata = pd.read_csv("metadata.tsv", sep="\t", index_col=0)
print(metadata.shape)


rule all:
    input:
        expand("embeddings/GPN_avgpool_{pool}.npy", pool=[2000, 500, 100])


rule make_h5ad:
    input:
        "input/GSE80744_ath1001_tx_norm_2016-04-21-UQ_gNorm_normCounts_k4.tsv.gz",
    output:
        "adata.h5ad",
    run:
        df = pd.read_csv(input[0], sep="\t", index_col=0).T
        print(df)
        df_indices = df.index.str[1:].astype(int).values
        print(df_indices.shape)
        mask = np.isin(df_indices, metadata.index.values)
        df_indices = df_indices[mask]
        df = df.iloc[mask]
        print(df_indices.shape)
        X = df.values
        obs = metadata.loc[df_indices]
        print(obs)
        var = pd.DataFrame(index=df.columns)
        adata = ad.AnnData(X=X, obs=obs, var=var)
        print(adata)
        adata.write(output[0], compression="gzip")


rule make_dataset:
    input:
        "adata.h5ad",
        "../vep/tss.bed",
        "../mlm/tair10.fa",
        "1001_SNP_MATRIX/imputed_snps_binary.hdf5",
        "../vep/variants/all.parquet",
    output:
        "dataset.parquet",
    run:
        adata = ad.read_h5ad(input[0])
        #sc.pp.log1p(adata)
        #print(adata.shape)
        # don't remember where these came from. maybe their paper.
        marker_genes = ["AT2G27120", "AT1G47920", "AT1G78330", "AT2G47680", "AT3G54460", "AT5G63120", "AT1G09860", "AT4G03050", "AT1G04880", "AT4G39590",]
        adata = adata[:, marker_genes]
        #print(adata.shape)

        df = pd.read_csv(input[1], sep="\t", header=None, names=["chromosome", "start", "end", "transcript_id", "score", "strand"])
        #print(df)
        df["gene_id"] = df.transcript_id.str.split('.').str[0]
        #print(df)
        df = df[df.gene_id.isin(marker_genes)]
        #print(df)
        # TODO: some are missing, need to figure out why
        # e.g. AT1G78330. it's a pseudogene. can skip for now.

        # filter to single-TSS genes
        gene_id_counts = df.gene_id.value_counts().to_frame().rename(columns=dict(gene_id='gene_id_count'))
        df = df.merge(gene_id_counts, how="left", left_on="gene_id", right_index=True)
        df = df[df.gene_id_count==1]
        #print(df)

        # TODO: make sure there's not a +-1 discrepancy between genes on
        # positive or negative strand
        df.start -= CONTEXT
        df.end += CONTEXT-1

        genome = SeqIO.to_dict(SeqIO.parse(input[2], "fasta"))

        def get_seq(row):
            return str(genome[row.chromosome][row.start:row.end].seq)

        def check_reverse_complement(row):
            if row.strand == "+":
                return row.seq
            elif row.strand == "-":
                return str(Seq(row.seq).reverse_complement())

        df["seq"] = df.apply(get_seq, axis=1)
        #print(df)

        snps = h5py.File(input[3],'r')
        # Get all SNP positions for all chromosomes (len=10709949)
        positions = snps['positions'][:]
        #print(len(positions))
        positions -= 1
        accessions = np.array([accession.decode('UTF-8') for accession in snps['accessions'][:]])
        # Array of tuples with start/stop indices for each chromosome
        chr_regions = snps['positions'].attrs['chr_regions']
        
        variants = pd.read_parquet(input[4])  # to get ref and alt
        #print(variants)

        rows = []

        for _, tss in tqdm(df.iterrows(), total=df.shape[0]):
            #print(tss.gene_id)
            chr_indices = chr_regions[int(tss.chromosome[-1])-1]
            chr_positions = positions[chr_indices[0]:chr_indices[1]]
            mask_tss_positions = (chr_positions > tss.start) & (chr_positions < tss.end)
            tss_positions = chr_positions[mask_tss_positions]
            #print(len(tss_positions))
            if len(tss_positions) == 0: continue
            tss_variants = variants[(variants.chromosome == tss.chromosome) & (variants.pos > tss.start) & (variants.pos < tss.end)].set_index("pos").loc[tss_positions]
            refs = tss_variants.ref.values
            alts = tss_variants.alt.values
            tss_relative_positions = tss_positions - tss.start  # need to reverse complement this. maybe reverse complement at the end
            seq = np.array(list(tss.seq))
            assert((refs==seq[tss_relative_positions])).all()

            snp_matrix = snps["snps"][chr_indices[0] + np.where(mask_tss_positions)[0]]

            # TODO: snp_matrix often has 1 or 2 less singletons, maybe because it includes imputation?
            # overall they agree though
            #print((snp_matrix.sum(axis=1)==1).sum(), (tss_variants.AC==2).sum())

            for accession in adata.obs.index.values:
                idx_accession = np.where(accessions==accession)[0]
                accession_snps = snp_matrix[:, idx_accession].ravel().astype(bool)
                seq_accession = seq.copy()
                seq_accession[tss_relative_positions[accession_snps]] = alts[accession_snps]
                new_row = tss.copy()
                new_row["accession"] = accession
                new_row["seq"] = ''.join(seq_accession)
                new_row["expression"] = adata[accession, tss.gene_id].X[0,0]
                rows.append(new_row)
            
        dataset = pd.concat(rows, axis=1).T.reset_index(drop=True)
        print(dataset)
        dataset = dataset.merge(adata.obs[["admixture_group"]], how="left", left_on="accession", right_index=True)
        print(dataset)
        dataset.admixture_group = dataset.admixture_group.astype(str)
        print(dataset.columns)
        dataset["seq"] = dataset.apply(check_reverse_complement, axis=1)
        dataset.to_parquet(output[0], index=False)


class AverageModelAcrossSpace(nn.Module):
    def __init__(self, model_path, pool):
        super().__init__()
        self.model = ConvNetModel.from_pretrained(model_path)
        self.pooler = nn.AvgPool1d(kernel_size=pool, stride=pool)

    def forward(self, input_ids=None, **kwargs):
        x = self.model(input_ids=input_ids).last_hidden_state
        batch_size = len(x)
        return self.pooler(x.transpose(1,2)).reshape(batch_size, -1)


rule embed_seqs_model:
    input:
        "dataset.parquet",
        directory("../../plantbert/mlm/results_512_convnet_ftAth_alone/checkpoint-800000"),
    output:
        "embeddings/GPN_avgpool_{pool}.npy",
    threads:
        workflow.cores
    run:
        dataset = load_dataset('parquet', data_files={"test": input[0]})["test"]
        print(dataset)

        tokenizer = AutoTokenizer.from_pretrained(input[1])
        print(tokenizer)

        def encode(examples):
            return tokenizer(examples['seq'], return_token_type_ids=False, return_attention_mask=False,)

        dataset = dataset.map(encode, batched=True)
        print(dataset)

        model = AverageModelAcrossSpace(input[1], int(wildcards["pool"]))

        training_args = TrainingArguments(
            output_dir=tempfile.TemporaryDirectory().name,
            per_device_eval_batch_size=128,
            dataloader_num_workers=0,
        )
        trainer = Trainer(model=model, args=training_args)
        embedding = trainer.predict(test_dataset=dataset).predictions
        print(embedding.shape)
        np.save(output[0], embedding)


rule embed_seqs_one_hot:
    input:
        "dataset.parquet",
    output:
        "embeddings/one_hot.npy",
    run:
        dataset = pd.read_parquet(input[0])
        embedding = OneHotEncoder(sparse=False).fit_transform(np.concatenate(dataset.seq.map(list).map(np.array).values).reshape(-1, 1)).reshape(len(dataset), -1)
        print(embedding.shape)
        np.save(output[0], embedding)
