from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio import SeqIO, bgzf
import gzip
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
tqdm.pandas(desc="my bar!")

import matplotlib
matplotlib.use('pdf')


configfile: "config.yaml"


tracks = pd.read_csv(config["tracks_path"], sep="\t", index_col=0)
tracks = tracks[~tracks.index.str.startswith("DGF")]
print(tracks)

REGION_SIZE = 200
FLANK_SIZE = 400
TOTAL_SIZE = REGION_SIZE + 2 * FLANK_SIZE

splits = ["train", "val", "test"]

genomes = pd.read_csv("genome_list.tsv", header=None).values.ravel()
print(genomes)

rule all:
    input:
        "genomes/all.contigs.fa.gz",
        "windows/val/1000/1000/seqs.txt",
        "tair10.train.fa",
        #expand("genomes/{genome}.contigs.fa.gz", genome=genomes),
        #"intersection.seq.bed",
        #"plots/positive_proportion.pdf",
        #expand("datasets/{split}.parquet", split=splits),
        #"pos_regions/intersection.bed",
        #expand("pos_regions/{split}/intersection.bed", split=splits),
        #"all.bed",
        #"plots/n_tracks_per_feature_type.pdf",
        #expand("peaks/{track_name}.bed", track_name=tracks.index),
        #expand("processed_peaks/{track_name}.bed", track_name=tracks.index),
        #expand("pos_regions/{track_name}/all.bed", track_name=tracks.index),
        #expand("neg_regions/{track_name}/{split}.bed", track_name=tracks.index, split=splits),
        #"neg_regions/all/intersection.bed",


rule download_track:
    output:
        "peaks/{track_name}.bed",
    params:
        track_url = lambda wildcards: tracks.loc[wildcards["track_name"]].track_url
    shell:
        "wget {params.track_url} -O {output}"


rule plot_track_stats:
    input:
        expand("peaks/{track_name}.bed", track_name=tracks.index),
    output:
        "plots/n_tracks_per_feature_type.pdf",
        "plots/n_peaks_per_track.pdf",
        "plots/peak_width.pdf",
    run:
        df1s = []
        df2s = []
        for i, track_name in enumerate(tracks.index):
            if i % 10 == 0: print(i)
            df = pd.read_csv(input[i], sep="\t", header=None, usecols=[0, 1, 2])
            df["peak_width"] = df[2] - df[1]
            df["track_name"] = track_name
            df = df[["track_name", "peak_width"]]
            df1s.append(df)
            df2s.append(df.groupby("track_name").size().to_frame("n_peaks").reset_index())
        df1 = pd.concat(df1s, ignore_index=True)
        df2 = pd.concat(df2s, ignore_index=True)
        for df in [df1, df2]:
            df["feature_type"] = df.track_name.str.split("_").str[0]
            df.feature_type.replace({
                "DHS": "DNase I hypersensitive site",
                "DGF": "DNase I digital genomic footprinting",
                "HM": "Histone modification",
                "TFBS": "TF binding site",
               }, inplace=True)
        print(df1)
        print(df2)

        sns.countplot(data=df2, y="feature_type")
        plt.savefig(output[0], bbox_inches='tight')
        plt.close()

        #sns.displot(data=df2, col="feature_type", x="n_peaks", bins=20, facet_kws=dict(sharex=False, sharey=False))
        #plt.savefig(output[1], bbox_inches='tight')
        #plt.close()

        #sns.displot(data=df1, col="feature_type", x="peak_width", bins=20,  facet_kws=dict(sharex=False, sharey=False))
        #plt.savefig(output[2], bbox_inches='tight')
        #plt.close()

        #sns.displot(data=df2, col="feature_type", x="n_peaks", kind="kde", height=3, facet_kws=dict(sharex=False, sharey=False))
        #plt.savefig(output[1], bbox_inches='tight')
        #plt.close()

        #sns.displot(data=df1, col="feature_type", x="peak_width", kind="kde", height=3, aspect=2, facet_kws=dict(sharex=False, sharey=False))
        #plt.savefig(output[2], bbox_inches='tight')
        #plt.close()

        #sns.boxplot(data=df2, y="feature_type", x="n_peaks")
        #plt.savefig(output[1], bbox_inches='tight')
        #plt.close()

        #sns.boxplot(data=df1, y="feature_type", x="peak_width")
        #plt.savefig(output[2], bbox_inches='tight')
        #plt.close()

        g = sns.catplot(data=df2, row="feature_type", y="n_peaks", kind="box", height=3, sharex=False, sharey=False, showfliers=False)
        g.set_titles(col_template="{col_name}", row_template="{row_name}")
        plt.savefig(output[1], bbox_inches='tight')
        plt.close()

        g = sns.catplot(data=df1, row="feature_type", y="peak_width", kind="box", height=3, sharex=False, sharey=False, showfliers=False)
        g.set_titles(col_template="{col_name}", row_template="{row_name}")
        plt.savefig(output[2], bbox_inches='tight')
        plt.close()


rule process_peaks:
    input:
        "peaks/{track_name}.bed",
        config["chrom_sizes_path"],
    output:
        "processed_peaks/{track_name}.bed",
    run:
        bed = pd.read_csv(input[0], sep="\t", header=None, usecols=[0, 1, 2]).rename(columns={0: "chromosome", 1: "start", 2: "end"})
        chrom_sizes = pd.read_csv(input[1], sep="\t", header=None, index_col=0)
        bed = bed[bed.chromosome.isin(chrom_sizes.index.values)]
        bed["track_name"] = wildcards["track_name"]
        bed.to_csv(output[0], sep="\t", header=False, index=False)


rule merge_tracks:
    input:
        expand("processed_peaks/{track_name}.bed", track_name=tracks.index),
    output:
        "processed_peaks/all.bed",
    shell:
        "cat {input} | bedtools sort -i stdin > {output}"


rule make_windows:
    input:
        config["chrom_sizes_path"],
    output:
        "windows.bed"
    shell:
        "bedtools makewindows -g {input} -w {REGION_SIZE} > {output}"


rule bedtools_intersect_and_slop:
    input:
        "windows.bed",
        "processed_peaks/all.bed",
        config["chrom_sizes_path"],
    output:
        "intersection.bed",
    shell:
        """bedtools map -a {input[0]} -b {input[1]} -f 0.5 -o distinct -c 4 | bedtools slop -i stdin -g {input[2]} -b {FLANK_SIZE} | awk '$3-$2 == {TOTAL_SIZE}' > {output}"""


rule download_reference:
    output:
        "tair10.raw.fa",
    shell:
        "wget https://www.arabidopsis.org/download_files/Genes/TAIR10_genome_release/TAIR10_chromosome_files/TAIR10_chr_all.fas -O {output}"


rule clean_fasta:
    input:
        "tair10.raw.fa",
    output:
        "tair10.fa",
    run:
        records = []
        with open(input[0]) as input_handle:
            for record in SeqIO.parse(input_handle, "fasta"):
                print(record.id)
                if record.id in ["M", "chloroplast"]: continue
                record.id = "Chr" + record.id
                print(record.id)
                records.append(record)

        with open(output[0], "w") as output_handle:
            SeqIO.write(records, output_handle, "fasta")


rule add_seq:
    input:
        "intersection.bed",
        "tair10.fa",
    output:
        "intersection.seq.bed",
    shell:
        "bedtools getfasta -fi {input[1]} -bed {input[0]} -tab -bedOut > {output}"


rule make_dataset:
    input:
        "intersection.seq.bed",
    output:
        expand("datasets/{split}.parquet", split=splits),
    run:
        track_list = tracks.index.values.tolist()

        regions = pd.read_csv(input[0], sep="\t", header=None, names=["chromosome", "start", "end", "features", "seq"])
        print(regions.shape)
        regions = regions[~(regions.seq.str.contains("[^ACTG]"))]
        print(regions.shape)
        regions = regions.sample(frac=1, random_state=42)
        regions["strand"] = "+"
        regions = regions[["chromosome", "start", "end", "strand", "seq", "features"]]
        regions.loc[:, track_list] = np.uint8(0)
        regions.features = regions.features.apply(lambda x: x.split(','))
        for index, row in tqdm(regions.iterrows(), total=regions.shape[0]):
            if not '.' in row.features:  # this means not empty
                regions.loc[index, row.features] = np.uint8(1)
        regions.drop(columns="features", inplace=True)
        print(regions)

        regions_rc = regions.copy()
        regions_rc["strand"] = "-"
        regions_rc["seq"] = regions_rc.seq.apply(lambda x: str(Seq(x).reverse_complement()))

        regions = pd.concat([regions, regions_rc], ignore_index=True)
        print(regions)
        print(regions.columns)

        for i, split in enumerate(splits):
            if split == "train":
                mask = (regions.chromosome.isin(["Chr1", "Chr2", "Chr3"]) | ((regions.chromosome=="Chr4") & (regions.start < 12000000)))
            elif split == "val":
                mask = ((regions.chromosome=="Chr4") & (regions.start >= 12000000))
            elif split == "test":
                mask = (regions.chromosome=="Chr5")
            print(split, mask.mean())
            regions[mask].to_parquet(output[i], index=False)


rule plot_class_balance:
    input:
        "datasets/test.parquet",
    output:
        "plots/positive_proportion.pdf",
        "plots/log10_positive_proportion.pdf",
    run:
        d = pd.read_parquet(input[0])
        features = [col for col in d.columns.values if col not in ["chromosome", "start", "end", "strand", "seq"]]
        p = d[features].mean().to_frame("positive_proportion")
        print(p)
        p["feature_type"] = p.index.str.split("_").str[0]
        p.feature_type.replace({
            "DHS": "DNase I hypersensitive site",
            "DGF": "DNase I digital genomic footprinting",
            "HM": "Histone modification",
            "TFBS": "TF binding site",
        }, inplace=True)
        print(p)
        p["log10_positive_proportion"] = p.positive_proportion.apply(np.log10)
        print(p)

        g = sns.catplot(data=p, row="feature_type", y="positive_proportion", kind="box", height=3, sharex=False, sharey=False, showfliers=False)
        g.set_titles(col_template="{col_name}", row_template="{row_name}")
        plt.savefig(output[0], bbox_inches='tight')
        plt.close()

        g = sns.catplot(data=p, row="feature_type", y="log10_positive_proportion", kind="box", height=3, sharex=False, sharey=False, showfliers=False)
        g.set_titles(col_template="{col_name}", row_template="{row_name}")
        plt.savefig(output[1], bbox_inches='tight')


rule make_windows_lm_raw:
    input:
       "intervals/{split}.bed"
    output:
        "windows/{split}/{window_size}/{step_size}/raw.bed"
    shell:
        "bedtools makewindows -b {input} -w {wildcards.window_size} -s {wildcards.step_size} > {output}"


rule make_windows_lm_pos:
    input:
        "{anything}/raw.bed"
    output:
        "{anything}/pos.bed"
    shell:
        """awk '{{print $0 "\t.\t.\t+"}}' {input} > {output}"""


rule make_windows_lm_neg:
    input:
        "{anything}/raw.bed"
    output:
        "{anything}/neg.bed"
    shell:
        """awk '{{print $0 "\t.\t.\t-"}}' {input} > {output}"""


rule make_windows_lm:
    input:
        "{anything}/pos.bed",
        "{anything}/neg.bed",
    output:
        "{anything}/all.bed",
    shell:
        "cat {input} > {output}"


rule extract_seq_lm:
    input:
        "{anything}/all.bed",
        "tair10.fa",
    output:
        "{anything}/seqs.txt",
    shell:
        """bedtools getfasta -fi {input[1]} -bed {input[0]} -s | grep -v "[^ACTG]" > {output}"""


rule filter_fasta_train:
    input:
        "tair10.fa",
        "intervals/train.bed",
    output:
        "tair10.train.fa",
    run:
        intervals = pd.read_csv(input[1], sep="\t", header=None, names=["chromosome", "start", "end"]).set_index("chromosome")

        records = []
        with open(input[0]) as input_handle:
            for record in SeqIO.parse(input_handle, "fasta"):
                if record.id in intervals.index.values:
                    records.append(record[intervals.loc[record.id, "start"]:intervals.loc[record.id, "end"]])

        with open(output[0], "w") as output_handle:
            SeqIO.write(records, output_handle, "fasta")


rule split_into_contigs:
    input:
        "{anything}.fa.gz",
    output:
        "{anything}.contigs.fa.gz",
    run:
        with gzip.open(input[0], "rt") as handle:
            sequence = '\n'.join([str(record.seq).strip() for record in SeqIO.parse(handle, "fasta")])
        m = re.sub('[^ACGTacgt]+', '\n', sequence).split('\n')

        m = [x for x in m if len(x) > 0]
        records = [SeqRecord(Seq(m[i]), id=f'contig_{i}', annotations={"molecule_type": "DNA"}) for i in range(len(m))]
        with bgzf.BgzfWriter(output[0], "wb") as outgz:
            SeqIO.write(sequences=records, handle=outgz, format="fasta")


rule concat_genomes:
    input:
        expand("genomes/{genome}.contigs.fa.gz", genome=genomes),
    output:
        "genomes/all.contigs.fa.gz",
    shell:
        "cat {input} > {output}"


rule download_vcf:
    output:
        "variants/all.vcf.gz",
    shell:
        "wget --no-check-certificate https://1001genomes.org/data/GMI-MPI/releases/v3.1/1001genomes_snp-short-indel_only_ACGTN.vcf.gz -O {output}"


rule filter_vcf:
    input:
        "variants/all.vcf.gz",
    output:
        "variants/filt.bed.gz",
    shell:
        "vcftools --gzvcf {input} --counts --out tmp --min-alleles 2 --max-alleles 2 --remove-indels && mv tmp.frq.count variants/filt.bed && gzip variants/filt.bed"


rule download_gff:
    output:
        "tair10.gff"
    shell:
        "wget https://www.arabidopsis.org/download_files/Genes/TAIR10_genome_release/TAIR10_gff3/TAIR10_GFF3_genes.gff -O {output}"


rule extract_TSS:
    input:
        "tair10.gff",
    output:
        "tss.bed",
    run:
        df = pd.read_csv(
            input[0], sep="\t", header=None, comment="#",
            names=['chromosome', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute']
        )
        print(df)
        df = df[df.feature=="mRNA"]
        df["transcript_id"] = df.attribute.str.extract(r'ID=([^;]*);')
        print(df)
        df = df.groupby("transcript_id").agg({"chromosome": "first", "start": "min", "end": "max", "strand": "first"}).reset_index()
        print(df)
        df.start = df.apply(lambda row: row.start if row.strand=="+" else row.end, axis=1)
        df.end = df.start + 1
        df["score"] = "."
        df = df.sort_values(["chromosome", "start"])
        df.to_csv(output[0], sep="\t", index=False, header=False, columns=["chromosome", "start", "end", "transcript_id", "score", "strand"])


rule process_variants:
    input:
        "variants/filt.bed.gz",
        "tair10.fa",
    output:
        "variants/filt.processed.bed.gz",
        "variants/coordinates.bed",
    run:
        variants = pd.read_csv(input[0], sep="\t", header=0, names=["chromosome", "pos", "N_ALLELES", "AN", "ref_count", "alt_count"]).drop(columns="N_ALLELES")
        print(variants)
        variants.chromosome = "Chr" + variants.chromosome.astype(str)
        variants.pos = variants.pos - 1  # vcf have 1-based coordinates by convention, while BioSeq and bedtools not
        print(variants)
        genome = SeqIO.to_dict(SeqIO.parse(input[1], "fasta"))

        def find_ref_alt_AC(row):
            ref = genome[row.chromosome][row.pos]
            assert(ref == row.ref_count[0])
            alt, AC = row.alt_count.split(":")
            AC = int(AC)
            return ref, alt, AC

        variants["ref"], variants["alt"], variants["AC"] = zip(*variants.apply(find_ref_alt_AC, axis=1))
        print(variants)
        variants.to_csv(output[0], sep="\t", index=False, columns=["chromosome", "pos", "ref", "alt", "AC", "AN"])
        variants["start"] = variants.pos
        variants["end"] = variants.start + 1
        variants.to_csv(output[1], sep="\t", index=False, header=False, columns=["chromosome", "start", "end"])


rule find_dist_to_TSS:
    input:
        "variants/coordinates.bed",
        "tss.bed",
    output:
        "dist_to_tss.txt",
    shell:
        "bedtools closest -a {input[0]} -b {input[1]} -D b -t first | cut -f 7,10 > {output}"


rule add_info_variants:
    input:
        "variants/filt.processed.bed.gz",
        "dist_to_tss.txt",
    output:
        "variants/all.parquet",
    run:
        variants = pd.read_csv(input[0], sep="\t")
        print(variants)
        dist_to_tss = pd.read_csv(input[1], sep="\t", header=None, names=["closest_TSS", "dist_to_TSS"])
        print(dist_to_tss)
        variants = pd.concat([variants, dist_to_tss], axis=1)
        print(variants)
        variants.to_parquet(output[0], index=False)


rule filter_variants:
    input:
        "variants/all.parquet",
    output:
        "variants/filt.parquet",
    run:
        df = pd.read_parquet(input[0])
        print(df)
        df = df[(df.chromosome=="Chr5") & (df.dist_to_TSS.abs() <= 1000)]
        print(df)
        df.to_parquet(output[0], index=False)
